{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Cómo funciona pytorch? \n",
    "Vamos a ver un tutorial rápido del tipo de datos de pytorch y cómo trabaja internamente esta librería. Para esto tendrás que haber seguido correctamente todos los pasos anteriores. Para esto necesitas la **versión interactiva del notebook**. \n",
    "\n",
    "Para esta sección: \n",
    "* **Abre Jupyter** (consultar arriba)\n",
    "* Navega hasta el notebook `00 Práctica Deep Learning - Introducción.ipynb` y ábrelo.\n",
    "* Baja hasta esta sección. \n",
    "\n",
    "Pero antes de nada os cuento algunas diferencias entre matlab y python: \n",
    "* Python es un **lenguaje de propósito general** mientras que matlab es un lenguaje **específico para ciencia e ingeniería**. Esto no es ni bueno ni malo; matlab es más fácil de utilizar para ingeniería sin preparación, pero python es más versátil. \n",
    "* Debido a ello, **Matlab carga automáticamente todas las funciones** mientras que en Python, **hay que cargar las librerías que vamos a utilizar**. Esto hace que usar funciones en matlab sea más sencillo (dos letras menos que escribir), pero a costa de que es más difícil gestionar la memoria, y los nombres de funciones se puden superponer. Supon que `A` es una matriz. Para hacer la pseudoinversa, en matlab hacemos: \n",
    "\n",
    "```matlab\n",
    "pinv(A)\n",
    "```\n",
    "* en python tenemos que cargar la librería:\n",
    "```python\n",
    "import scipy as sp\n",
    "sp.pinv(A)\n",
    "```\n",
    "* Esto genera una cosa llamada **espacio de nombres**, en el que las funciones de cada librería van precedidas por su abreviatura (si importamos con `import x as y`) o el propio nombre si usamos `import torch`, `torch.tensor()`, mientras que en matlab basta con llamar a la función. Por ejemplo, cuando en matlab escribimos:\n",
    "    - `vector = [1, 2, 3]`\n",
    "* en python+pytorch necesitamos especificar que es un tensor (un array multidimensional):\n",
    "    - `vector = torch.tensor([1,2,3])`\n",
    "\n",
    "Vamos a cargar la librería con `import torch` y ver que podemos, por ejemplo, construir una matriz de 5x3 aleatoria. Para ejecutar una celda, basta con seleccionarla (bien con las flechas del teclado, bien con el ratón) y pulsando `Ctrl+Enter` (o bien pulsando \"Run\" en la barra superior). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2472, 0.7132, 0.1375],\n",
      "        [0.7200, 0.2924, 0.3832],\n",
      "        [0.4341, 0.3518, 0.9204],\n",
      "        [0.3709, 0.2253, 0.6080],\n",
      "        [0.9817, 0.5234, 0.2136]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O una matriz de ceros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O a partir de unos datos dados, y podemos mostrarla con `print`, pero también acceder a sus características, como el tamaño de la matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 3.0000, 3.0000],\n",
      "        [2.0000, 1.0000, 5.0000],\n",
      "        [3.0000, 4.0000, 2.0000],\n",
      "        [7.0000, 6.0000, 5.0000],\n",
      "        [2.0000, 1.0000, 2.0000]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[5.5, 3, 3],[2,1, 5], [3,4,2],[7,6,5],[2,1,2]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con tensores se puede operar de forma normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.3343, 3.7080, 3.9404],\n",
      "        [2.3815, 1.0040, 5.7915],\n",
      "        [3.0152, 4.8507, 2.5595],\n",
      "        [7.2281, 6.1131, 5.3825],\n",
      "        [2.3290, 1.9387, 2.1796]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero OJO CUIDAO, tienen que ser del mismo tamaño, si no, va a dar error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-42fe1c64fd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "y = torch.rand(2,3)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede hacer *slicing* como en numpy o Matlab. Por ejemplo, para extraer la primera columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 1., 4., 6., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra característica que nos será de mucha utilidad es cambiar la forma de la matriz, que en otros lenguajes se conoce como `reshape`, y aquí es un método del objeto tensor llamado `view()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos operar con tensores y valores escalares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9619, 1.8762, 2.1108, 2.6227],\n",
      "        [2.4012, 2.8760, 0.8042, 1.2621],\n",
      "        [1.2319, 2.2896, 1.6425, 3.2375],\n",
      "        [3.3435, 2.0807, 3.2948, 0.7555]])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos definir funciones que realicen estas operaciones que apliquemos a los diferentes tensores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1142, 1.8803, 2.1137, 2.6956],\n",
      "        [2.4345, 3.0064, 1.4411, 1.4620],\n",
      "        [1.4517, 2.3079, 1.6809, 3.4660],\n",
      "        [3.6033, 2.0823, 3.5401, 1.4559]])\n"
     ]
    }
   ],
   "source": [
    "def modulo(x,y):\n",
    "    aux = x**2 + y**2\n",
    "    salida = torch.sqrt(aux)\n",
    "    return salida\n",
    "\n",
    "print(modulo(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, una parte fundamental es que pytorch conserva memoria de las operaciones realizadas en un vector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propiedad `grad_fn` será fundamental en el entrenamiento de redes neuronales, ya que guarda el gradiente de la operación o función que se haya aplicado a los datos. Esto se conserva a traves de todas las operaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O incluso llevan cuenta de las operaciones realizadas con funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1623, 3.1623],\n",
      "        [3.1623, 3.1623]], grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(modulo(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular el gradiente a lo largo de estas operaciones se utiliza la función `.backward()`, que realiza la propagación del gradiente hacia atrás. Podemos mostrar el gradiente $\\frac{\\partial out}{\\partial x}$ con la propiedad `x.grad`, así que lo vemos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habrá aquí una matriz de 2x2 con valores 4.5. Si llamamos el tensor de salida $o$, tenemos que:\n",
    "\n",
    "$$ \n",
    "o = \\frac{1}{4} \\sum_iz_i, \\quad z_i = 3(x_i + 2)^2\n",
    "$$\n",
    "\n",
    "Así que $z_i|_{x_i=1} = 27$. Entonces, la $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$ y $\\frac{\\partial o}{\\partial x_i} |_{x_i=1} = \\frac{9}{2} = 4.5$\n",
    "\n",
    "Gracias a esto, y a las matemáticas del algoritmo de propagación hacia atrás (*backpropagation*, ver video de introducción a la práctica), se pueden actualizar los pesos en función de una función de pérdida en las redes neuronales. Se puede activar y desactivar el cálculo del gradiente con la expresión `torch.no_grad()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la próxima sección, `01 Práctica Deep Learning - Perceptrón Multicapa.ipynb`, veremos como se construye y se entrena nuestra primera red neuronal utilizando estas características de pytorch. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}